import os
import requests
import json
import wave
import base64
from glob import glob
import subprocess
from openai import OpenAI
from PIL import Image
from natsort import natsorted

# === è¨­å®š ===
SLIDE_IMG_DIR = "slides"
AUDIO_DIR = "audios"
VIDEO_DIR = "videos"
SCRIPT_DIR = "scripts"
FINAL_VIDEO = "lecture_video.mp4"
VOICEVOX_SPEAKER_ID = 3
OPENAI_API_KEY = "sk-**************"

client = OpenAI(api_key=OPENAI_API_KEY)

# === ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ ===
for folder in [SLIDE_IMG_DIR, AUDIO_DIR, VIDEO_DIR, SCRIPT_DIR]:
    os.makedirs(folder, exist_ok=True)

# === VOICEVOXéŸ³å£°ç”Ÿæˆ ===
def generate_voice(text, filename, speaker_id=VOICEVOX_SPEAKER_ID):
    base_url = "http://localhost:50021"
    query = requests.post(f"{base_url}/audio_query", params={"text": text, "speaker": speaker_id}).json()
    query["speedScale"] = 1.1  # â† ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1.0ã€ã“ã‚Œã§30%æ—©ããªã‚Šã¾ã™
    synthesis = requests.post(
        f"{base_url}/synthesis",
        params={"speaker": speaker_id},
        data=json.dumps(query),
        headers={"Content-Type": "application/json"}
    )
    with open(filename, "wb") as f:
        f.write(synthesis.content)

# === .wavã®é•·ã•å–å¾— ===
def get_wav_duration(wav_path):
    with wave.open(wav_path, 'rb') as wf:
        return wf.getnframes() / float(wf.getframerate())

def append_silence_to_audio(input_wav, output_wav, silence_duration=1.0):
    temp_output = output_wav + ".temp.wav"
    silence = "anullsrc=r=44100:cl=stereo"
    subprocess.run([
        "ffmpeg", "-y",
        "-i", input_wav,
        "-f", "lavfi", "-t", str(silence_duration), "-i", silence,
        "-filter_complex", "[0:a][1:a]concat=n=2:v=0:a=1[out]",
        "-map", "[out]",
        temp_output
    ], check=True)
    os.replace(temp_output, output_wav)  # â† ä¸Šæ›¸ã

# === ã‚¹ãƒ©ã‚¤ãƒ‰ç”»åƒ + éŸ³å£° â†’ å‹•ç”» ===
def create_slide_video(image_file, audio_file, output_file):
    duration = get_wav_duration(audio_file)
    subprocess.run([
        "ffmpeg", "-y",
        "-loop", "1",
        "-t", f"{duration:.2f}",
        "-i", image_file,
        "-i", audio_file,
        "-c:v", "libx264",
        "-r", "30",
        "-pix_fmt", "yuv420p",
        "-c:a", "aac",
        "-b:a", "192k",
        "-shortest",
        output_file
    ], check=True)

# === ãƒ¡ã‚¤ãƒ³å‡¦ç† ===
slide_images = natsorted(glob(f"{SLIDE_IMG_DIR}/ã‚¹ãƒ©ã‚¤ãƒˆã‚™*.png"))
for i, img_path in enumerate(slide_images, 1):
    print(f"â–¶ Slide {i} - GPTç”»åƒè§£æï¼†ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆä¸­...")

    with open(img_path, "rb") as f:
        b64_image = base64.b64encode(f.read()).decode("utf-8")

    instruction = "Slide1ã«è©²å½“ã™ã‚‹ãŸã‚ã€æŒ¨æ‹¶ã¨å°å…¥ã‚’å«ã‚ã¦ãã ã•ã„ã€‚" if i == 1 else "ã™ã§ã«è¬›ç¾©ãŒå§‹ã¾ã£ã¦ã„ã‚‹å‰æã§ã€æŒ¨æ‹¶ã‚„å°å…¥ã¯ä¸è¦ã§ã™ã€‚"

    prompt = (
        "ã“ã®ç”»åƒã¯å¤§å­¦è¬›ç¾©ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã§ã™ã€‚ä»¥ä¸‹ã®è¦ä»¶ã§è¬›ç¾©ç”¨ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š\n"
        f"ãƒ»{instruction}\n"
        "ãƒ»ã‚ã„ã•ã¤ã¯è¦–è´è€…ãŒã„ã¤ã®æ™‚é–“è¦–è´ã™ã‚‹ã‹ã‚ã‹ã‚‰ãªã„ã®ã§ã€ã‚ã„ã•ã¤ã¯ä¸è¦ã§ã™ã€‚\n"
        "ãƒ»ã¾ãšæ•´å½¢æ–‡ï¼ˆæ¼¢å­—å«ã‚€è‡ªç„¶ãªè¬›ç¾©æ–‡ï¼‰ã‚’ç”Ÿæˆã—ã€ãã®å¾Œã«VOICEVOXã§æ­£ã—ãèª­ã¿ä¸Šã’ã‚‰ã‚Œã‚‹ã‚ˆã†ã«å¹³ä»®åã®ã€èª­ã¿ç”¨ã€æ–‡ã‚‚å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n"
        "ãƒ»èª­ã¿ç”¨ã«ã¯å¥èª­ç‚¹ã‚„è‡ªç„¶ãªåŒºåˆ‡ã‚Šã‚’å«ã‚ã€ç†Ÿèªã®èª¤èª­ã‚’é¿ã‘ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šã€Œè§£ææ‰‹æ³•ã€ã¯ã€ã‹ã„ã›ãã—ã‚…ã»ã†ã€ï¼‰ã€‚\n"
        "ãƒ»ã‚¹ãƒ©ã‚¤ãƒ‰å†…ã«å«ã¾ã‚Œã‚‹ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚„å†…ç·šç•ªå·ãªã©ã®å€‹äººé€£çµ¡æƒ…å ±ã¯èª­ã¿ä¸Šã’ãªã„ã§ãã ã•ã„ã€‚\n"
        "ãƒ»äººã®åå‰ã«æ•¬ç§°ã¯ä¸è¦ã§ã™ã€‚\n"
        "ãƒ»é•·ã„æ•°å¼ã®èª­ã¿ä¸Šã’ã¯ã›ãšã€ã“ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã¨èª¬æ˜ã™ã‚Œã°è‰¯ã„ã§ã™ã€‚\n"
        "ãƒ»TeXå½¢å¼ã®æ•°å¼ãŒå«ã¾ã‚Œã‚‹å ´åˆã¯ã€ãã‚Œã‚’æ—¥æœ¬èªã®è‡ªç„¶ãªèª­ã¿æ–¹ã«ç›´ã—ã¦ãã ã•ã„ã€‚ä¾‹ãˆã°ã€\frac{1}{2}mv^2 ã¯ã€2åˆ†ã®1ã€ã‚¨ãƒ ã€ãƒ–ã‚¤ã®2ä¹—ã€ã¨ã„ã£ãŸå½¢ã«ã—ã¦ãã ã•ã„ã€‚\n"
        "ãƒ»TeXå½¢å¼ã«ãŠã„ã¦åˆ†æ•°ã¨å¾®åˆ†ã¯åŒºåˆ¥ã—ã¦èª­ã¿ä¸Šã’ã¦ãã ã•ã„ã€‚\n"
        "å‡ºåŠ›å½¢å¼ï¼š\næ•´å½¢æ–‡:\n[æ¼¢å­—å«ã‚€æ–‡]\n\nèª­ã¿ç”¨:\n[ã™ã¹ã¦ã²ã‚‰ãŒãª]\n"
    )

    messages = [
        {"role": "user", "content": [
            {"type": "text", "text": prompt},
            {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{b64_image}"}}
        ]}
    ]

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=0.4
    )
    content = response.choices[0].message.content.strip()

    lines = content.splitlines()
    kanji_lines = []
    yomi_lines = []
    mode = None
    for line in lines:
        if line.strip().startswith("æ•´å½¢æ–‡"):
            mode = "kanji"
        elif line.strip().startswith("èª­ã¿ç”¨"):
            mode = "yomi"
        elif mode == "kanji":
            kanji_lines.append(line)
        elif mode == "yomi":
            yomi_lines.append(line)

    kanji_script = "\n".join(kanji_lines).strip()
    yomi_script = "\n".join(yomi_lines).strip()

    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¿å­˜
    with open(f"{SCRIPT_DIR}/slide{i:03}_kanji.txt", "w", encoding="utf-8") as f:
        f.write(kanji_script)
    with open(f"{SCRIPT_DIR}/slide{i:03}_yomi.txt", "w", encoding="utf-8") as f:
        f.write(yomi_script)

    # éŸ³å£°ç”Ÿæˆ
    audio_path = f"{AUDIO_DIR}/slide{i:03}.wav"
    print(f"ğŸ”Š VOICEVOX éŸ³å£°ç”Ÿæˆä¸­ï¼ˆslide{i:03}.wavï¼‰")
    generate_voice(yomi_script, audio_path)
    append_silence_to_audio(audio_path, audio_path, silence_duration=1.0)

    # å‹•ç”»ç”Ÿæˆ
    video_path = f"{VIDEO_DIR}/slide{i:03}.mp4"
    print(f"ğŸ¬ ã‚¹ãƒ©ã‚¤ãƒ‰å‹•ç”»ç”Ÿæˆä¸­ï¼ˆslide{i:03}.mp4ï¼‰")
    create_slide_video(img_path, audio_path, video_path)

# === å…¨ã‚¹ãƒ©ã‚¤ãƒ‰å‹•ç”»ã®çµåˆ ===
print("ğŸ“¦ lecture_video.mp4 ã«çµåˆä¸­...")
with open("video_list.txt", "w") as f:
    for file in sorted(glob(f"{VIDEO_DIR}/slide*.mp4")):
        f.write(f"file '{file}'\n")

subprocess.run([
    "ffmpeg", "-y", "-f", "concat", "-safe", "0", "-i", "video_list.txt",
    "-c", "copy", FINAL_VIDEO
], check=True)

print(f"âœ… è¬›ç¾©å‹•ç”»ã®å®Œæˆï¼ãƒ•ã‚¡ã‚¤ãƒ«å: {FINAL_VIDEO}")
